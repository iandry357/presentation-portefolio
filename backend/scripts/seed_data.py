#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
seed_data.py - Seed + embeddings Voyage AI + insertion directe PostgreSQL
GÃ©nÃ¨re aussi init.sql en backup
"""

import json
import os
import sys
from datetime import datetime
from typing import List, Dict, Tuple
import time
import hashlib
import argparse

try:
    from voyageai import Client as VoyageClient
    import psycopg2
    from psycopg2.extensions import adapt, AsIs
except ImportError as e:
    print(f"Paquets manquants : {e}")
    print("pip install voyageai psycopg2-binary")
    sys.exit(1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

VOYAGE_MODEL       = "voyage-3"              # â†’ 1024 dim fixe
EMBEDDING_DIM      = 1024
BATCH_SIZE_EMBED   = 1
SLEEP_BETWEEN_CALLS = 25

# Racine du projet dans le conteneur
PROJECT_ROOT = "/app"

DATA_DIR = os.path.join(PROJECT_ROOT, "scripts", "data")

FILES = {
    "experiences": os.path.join(DATA_DIR, "experiences.json"),
    "formations":  os.path.join(DATA_DIR, "formations.json"),
    "skills":      os.path.join(DATA_DIR, "skills.json"),
}

OUTPUT_SQL = os.path.join(PROJECT_ROOT, "scripts", "init.sql")
EMBEDDINGS_CACHE = os.path.join(PROJECT_ROOT, "scripts", "embeddings_cache.json")

DB_PARAMS = {
    "host":     os.getenv("POSTGRES_HOST", "localhost"),
    "port":     int(os.getenv("POSTGRES_PORT", "5432")),
    "dbname":   os.getenv("POSTGRES_DB", "portfolio_rag"),
    "user":     os.getenv("POSTGRES_USER", "postgres"),
    "password": os.getenv("POSTGRES_PASSWORD", "postgres"),
    "client_encoding": "UTF8", 
}

VOYAGE_API_KEY = os.getenv("VOYAGE_API_KEY")
if not VOYAGE_API_KEY:
    print("VOYAGE_API_KEY manquante dans l'environnement")
    sys.exit(1)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def pg_quote(value):
    """
    Ã‰chappe proprement pour PostgreSQL via psycopg2.adapt()
    """
    if value is None or value == "":
        return "NULL"

    value = value.encode('ascii', errors='ignore').decode('ascii')
    adapted = adapt(value)
    # Gestion robuste de l'encodage pour caractÃ¨res franÃ§ais
    quoted = adapted.getquoted()
    if isinstance(quoted, bytes):
        try:
            return quoted.decode('utf-8')
        except UnicodeDecodeError:
            return quoted.decode('latin-1')
    return str(quoted)


def pg_array(arr: List[str]) -> str:
    """
    Transforme une liste Python en ARRAY PostgreSQL
    """
    if not arr:
        return "ARRAY[]::text[]"
    escaped = [pg_quote(item).strip("'") for item in arr]
    return "ARRAY[" + ",".join(f"'{e}'" for e in escaped) + "]::text[]"


def pg_vector(vec: List[float]) -> str:
    """
    Transforme une liste de floats en vecteur pgvector
    """
    return f"'[{','.join(f'{x:.7f}' for x in vec)}]'::vector"


def load_json(path: str) -> list:
    if not os.path.exists(path):
        print(f"Fichier absent : {path}")
        return []
    with open(path, encoding="utf-8") as f:
        return json.load(f)


def normalize_skill_key(s: Dict) -> Tuple[str, str, str]:
    name = (s.get("name") or "").strip().lower()
    cat  = (s.get("category") or "Autres").strip()
    lvl  = (s.get("proficiency_level") or "IntermÃ©diaire").strip()
    return name, cat, lvl


def collect_unique_skills(exps: List, forms: List, globals_sk: List) -> List[Dict]:
    seen = {}
    for source in [globals_sk, *[e.get("skills", []) for e in exps], *[p.get("skills", []) for e in exps for p in e.get("projects", [])]]:
        for item in source:
            if isinstance(item, str):
                sk = {"name": item.strip(), "category": "Programmation", "proficiency_level": "IntermÃ©diaire"}
            else:
                sk = item
            key = normalize_skill_key(sk)
            if key not in seen:
                seen[key] = sk
    return list(seen.values())


def text_experience(exp: Dict) -> str:
    parts = [
        f"Entreprise : {exp.get('company','')}",
        f"RÃ´le : {exp.get('role','')}",
        f"Type : {exp.get('mission_type','')}",
        exp.get("context", ""),
        f"Technologies : {', '.join(exp.get('technologies',[]))}",
    ]
    return " | ".join(filter(None, parts)) or "ExpÃ©rience professionnelle"


def text_project(proj: Dict) -> str:
    parts = [
        f"Projet : {proj.get('name','')}",
        proj.get("description", ""),
        f"Objectif : {proj.get('objective','')}",
        f"ProblÃ¨me : {proj.get('problem','')}",
        f"Solution : {proj.get('solution','')}",
        f"RÃ©sultats : {proj.get('results','')}",
        f"Impact : {proj.get('impact','')}",
        f"Stack : {proj.get('stack','')}",
    ]
    return " | ".join(filter(None, parts)) or "Projet"


def text_formation(form: Dict) -> str:
    parts = [
        f"{form.get('degree','')} Ã  {form.get('institution','')}",
        f"Domaine : {form.get('field','')}",
        form.get("description", ""),
        f"Apprentissages clÃ©s : {form.get('key_learnings','')}",
    ]
    return " | ".join(filter(None, parts)) or "Formation"


def text_hash(text: str) -> str:
    """
    GÃ©nÃ¨re un hash MD5 du texte pour cache
    """
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def load_embeddings_cache() -> Dict:
    """
    Charge le cache d'embeddings depuis le fichier JSON
    """
    if not os.path.exists(EMBEDDINGS_CACHE):
        return {}
    
    try:
        with open(EMBEDDINGS_CACHE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸  Erreur lecture cache : {e}")
        return {}


def save_embeddings_cache(cache: Dict):
    """
    Sauvegarde le cache d'embeddings dans le fichier JSON
    """
    try:
        with open(EMBEDDINGS_CACHE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
        print(f"âœ“ Cache sauvegardÃ© dans {EMBEDDINGS_CACHE}")
    except Exception as e:
        print(f"âš ï¸  Erreur sauvegarde cache : {e}")


def get_embeddings(texts: List[str], client: VoyageClient, use_cache: bool = True) -> List[List[float]]:
    if not texts:
        return []
    
    # Charger le cache existant
    cache = load_embeddings_cache() if use_cache else {}
    
    embs = []
    texts_to_compute = []
    indices_to_compute = []
    
    # VÃ©rifier quels textes sont dans le cache
    for i, text in enumerate(texts):
        text_id = text_hash(text)
        if use_cache and text_id in cache:
            embs.append(cache[text_id])
        else:
            embs.append(None)  # Placeholder
            texts_to_compute.append(text)
            indices_to_compute.append(i)
    
    # Calculer les embeddings manquants
    if texts_to_compute:
        print(f"ðŸ”„ Calcul de {len(texts_to_compute)} embeddings (cache: {len(texts) - len(texts_to_compute)})")
        
        computed_embs = []
        for i in range(0, len(texts_to_compute), BATCH_SIZE_EMBED):
            batch = texts_to_compute[i:i+BATCH_SIZE_EMBED]
            try:
                resp = client.embed(batch, model=VOYAGE_MODEL, input_type="document")
                computed_embs.extend(resp.embeddings)
            except Exception as e:
                print(f"âœ— Erreur embedding batch {i//BATCH_SIZE_EMBED +1}: {e}")
                computed_embs.extend([[0.0]*EMBEDDING_DIM for _ in batch])

            # Respecter le rate limit
            # if i + BATCH_SIZE_EMBED < len(texts_to_compute):
            #     time.sleep(20)
            time.sleep(SLEEP_BETWEEN_CALLS)
        # Mettre Ã  jour le cache et les rÃ©sultats
        for idx, computed_emb, text in zip(indices_to_compute, computed_embs, texts_to_compute):
            embs[idx] = computed_emb
            cache[text_hash(text)] = computed_emb
        
        # Sauvegarder le cache mis Ã  jour
        if use_cache:
            save_embeddings_cache(cache)
    else:
        print(f"âœ“ Tous les embeddings trouvÃ©s dans le cache")

    return embs


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  SQL generation avec bloc PL/pgSQL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_sql_file(exps, forms, all_skills, exp_embs, proj_embs, form_embs, proj_list):
    """
    GÃ©nÃ¨re init.sql avec bloc DO $$ pour utiliser des variables temporaires
    """
    lines = []
    lines.append(f"-- init.sql - gÃ©nÃ©rÃ© le {datetime.now():%Y-%m-%d %H:%M:%S}")
    lines.append("BEGIN;")
    lines.append("")
    
    # Bloc PL/pgSQL avec DECLARE
    lines.append("DO $$")
    lines.append("DECLARE")
    
    # DÃ©clarer variables pour experiences
    for i in range(len(exps)):
        lines.append(f"  exp_id_{i} INTEGER;")
    
    # DÃ©clarer variables pour projects
    for i in range(len(proj_list)):
        lines.append(f"  proj_id_{i} INTEGER;")
    
    lines.append("BEGIN")
    lines.append("")

    # 1. SKILLS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines.append("  -- 1. Skills (ON CONFLICT name)")
    for sk in all_skills:
        n = pg_quote(sk["name"])
        c = pg_quote(sk.get("category", "Autres"))
        l = pg_quote(sk.get("proficiency_level", "IntermÃ©diaire"))
        lines.append(
            f"  INSERT INTO skills (name, category, proficiency_level) "
            f"VALUES ({n}, {c}, {l}) ON CONFLICT (name) DO UPDATE "
            f"SET category = EXCLUDED.category, proficiency_level = EXCLUDED.proficiency_level;"
        )
    lines.append("")

    # 2. EXPERIENCES avec RETURNING INTO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines.append("  -- 2. Experiences")
    for i, exp in enumerate(exps):
        company = pg_quote(exp.get("company"))
        role = pg_quote(exp.get("role"))
        mission_type = pg_quote(exp.get("mission_type", ""))
        start_date = f"'{exp['start_date']}'" if exp.get("start_date") else "NULL"
        end_date = f"'{exp['end_date']}'" if exp.get("end_date") else "NULL"
        duration = exp.get("duration_months", "NULL")
        location = pg_quote(exp.get("location", ""))
        context = pg_quote(exp.get("context", ""))
        techs = pg_array(exp.get("technologies", []))
        embedding = pg_vector(exp_embs[i])
        
        lines.append(
            f"  INSERT INTO experiences (company, role, mission_type, start_date, end_date, "
            f"duration_months, location, context, technologies, embedding) "
            f"VALUES ({company}, {role}, {mission_type}, {start_date}, {end_date}, "
            f"{duration}, {location}, {context}, {techs}, {embedding}) "
            f"RETURNING id INTO exp_id_{i};"
        )

    lines.append("")

    # 3. FORMATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines.append("  -- 3. Formations")
    for i, form in enumerate(forms):
        inst = pg_quote(form.get("institution"))
        deg = pg_quote(form.get("degree"))
        field = pg_quote(form.get("field", ""))
        start_date = f"'{form['start_date']}'" if form.get("start_date") else "NULL"
        end_date = f"'{form['end_date']}'" if form.get("end_date") else "NULL"
        loc = pg_quote(form.get("location", ""))
        desc = pg_quote(form.get("description", ""))
        learn = pg_quote(form.get("key_learnings", ""))
        embedding = pg_vector(form_embs[i])
        
        lines.append(
            f"  INSERT INTO formations (institution, degree, field, start_date, end_date, "
            f"location, description, key_learnings, embedding) "
            f"VALUES ({inst}, {deg}, {field}, {start_date}, {end_date}, "
            f"{loc}, {desc}, {learn}, {embedding});"
        )

    lines.append("")

    # 4. PROJECTS + project_skills â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines.append("  -- 4. Projects + project_skills")
    for proj_idx, (exp_idx, proj) in enumerate(proj_list):
        name = pg_quote(proj.get("name"))
        desc = pg_quote(proj.get("description", ""))
        objective = pg_quote(proj.get("objective", ""))
        problem = pg_quote(proj.get("problem", ""))
        solution = pg_quote(proj.get("solution", ""))
        results = pg_quote(proj.get("results", ""))
        impact = pg_quote(proj.get("impact", ""))
        stack = pg_quote(proj.get("stack", ""))
        start_date = f"'{proj['start_date']}'" if proj.get("start_date") else "NULL"
        end_date = f"'{proj['end_date']}'" if proj.get("end_date") else "NULL"
        duration = proj.get("duration_months", "NULL")
        collabs = pg_quote(proj.get("collaborators", ""))
        proj_type = pg_quote(proj.get("project_type", ""))
        embedding = pg_vector(proj_embs[proj_idx])
        
        lines.append(
            f"  INSERT INTO projects (experience_id, name, description, objective, problem, "
            f"solution, results, impact, stack, start_date, end_date, duration_months, "
            f"collaborators, project_type, embedding) "
            f"VALUES (exp_id_{exp_idx}, {name}, {desc}, {objective}, {problem}, {solution}, "
            f"{results}, {impact}, {stack}, {start_date}, {end_date}, {duration}, "
            f"{collabs}, {proj_type}, {embedding}) "
            f"RETURNING id INTO proj_id_{proj_idx};"
        )

        # project_skills relations
        for sk_raw in proj.get("skills", []):
            sk_name = sk_raw if isinstance(sk_raw, str) else sk_raw.get("name", "")
            sk_name = sk_name.strip()
            if sk_name:
                sk_quoted = pg_quote(sk_name)
                lines.append(
                    f"  INSERT INTO project_skills (project_id, skill_id) "
                    f"  SELECT proj_id_{proj_idx}, id FROM skills WHERE name = {sk_quoted} "
                    f"  ON CONFLICT DO NOTHING;"
                )

    lines.append("")

    # 5. experience_skills â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines.append("  -- 5. experience_skills")
    for i, exp in enumerate(exps):
        for sk_raw in exp.get("skills", []):
            sk_name = sk_raw if isinstance(sk_raw, str) else sk_raw.get("name", "")
            sk_name = sk_name.strip()
            if sk_name:
                sk_quoted = pg_quote(sk_name)
                lines.append(
                    f"  INSERT INTO experience_skills (experience_id, skill_id) "
                    f"  SELECT exp_id_{i}, id FROM skills WHERE name = {sk_quoted} "
                    f"  ON CONFLICT DO NOTHING;"
                )

    lines.append("")
    lines.append("END $$;")
    lines.append("")
    lines.append("COMMIT;")

    sql_content = "\n".join(lines)
    
    # Sauvegarde backup
    with open(OUTPUT_SQL, "w", encoding="utf-8") as f:
        f.write(sql_content)
    print(f"âœ“ {OUTPUT_SQL} gÃ©nÃ©rÃ©")
    
    return sql_content


def execute_sql(sql_content: str):
    """
    ExÃ©cute le SQL directement dans PostgreSQL
    """
    try:
        conn = psycopg2.connect(**DB_PARAMS)
        conn.autocommit = False
        with conn.cursor() as cur:
            cur.execute(sql_content)
        conn.commit()
        print("âœ“ Insertion directe rÃ©ussie dans PostgreSQL")
    except Exception as e:
        print(f"âœ— Erreur lors de l'insertion : {e}")
        print(f"â†’ Utilisez manuellement : psql -f {OUTPUT_SQL}")
        sys.exit(1)
    finally:
        if 'conn' in locals():
            conn.close()


def main():
    # Parse arguments
    parser = argparse.ArgumentParser(description="Seed database avec embeddings Voyage AI")
    parser.add_argument('--force-recompute', action='store_true',
                       help="Forcer le recalcul de tous les embeddings (ignorer le cache)")
    args = parser.parse_args()
    
    use_cache = not args.force_recompute
    
    if args.force_recompute:
        print("âš ï¸  Mode --force-recompute : le cache sera ignorÃ©")
    
    print("ðŸ“‚ Lecture JSON...")
    exps  = load_json(FILES["experiences"])
    forms = load_json(FILES["formations"])
    g_sk  = load_json(FILES["skills"])

    if not any([exps, forms, g_sk]):
        print("âœ— Aucune donnÃ©e trouvÃ©e")
        return

    vo = VoyageClient(api_key=VOYAGE_API_KEY)

    # Skills uniques
    all_skills = collect_unique_skills(exps, forms, g_sk)
    print(f"âœ“ {len(all_skills)} compÃ©tences uniques")

    # Textes â†’ embeddings
    exp_texts  = [text_experience(e)  for e in exps]
    form_texts = [text_formation(f)   for f in forms]

    proj_list  = [(i, p) for i,e in enumerate(exps) for p in e.get("projects",[])]
    proj_texts = [text_project(p) for _,p in proj_list]

    print(f"ðŸ”¢ Embeddings Ã  calculer : exp={len(exp_texts)} | proj={len(proj_texts)} | form={len(form_texts)}")

    exp_embs  = get_embeddings(exp_texts,  vo, use_cache)
    proj_embs = get_embeddings(proj_texts, vo, use_cache)
    form_embs = get_embeddings(form_texts, vo, use_cache)

    # GÃ©nÃ©ration SQL
    sql_content = generate_sql_file(exps, forms, all_skills, exp_embs, proj_embs, form_embs, proj_list)
    
    # ExÃ©cution directe
    execute_sql(sql_content)


if __name__ == "__main__":
    main()